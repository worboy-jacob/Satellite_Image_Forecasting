{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class PovertyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset for satellite images with 4 channels: RGB + vegetation.\n",
    "    images_filenames: List of file paths to images (each presumably has 4 channels).\n",
    "    targets: List or array of poverty values (floats).\n",
    "    transform: Optional set of transforms (e.g., scaling, augmentations).\n",
    "    \"\"\"\n",
    "    def __init__(self, images_filenames, targets, transform=None):\n",
    "        self.images_filenames = images_filenames\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images_filenames[idx]\n",
    "        with Image.open(img_path) as img:\n",
    "            # TODO: Change loading to include all channels into the same numpy array\n",
    "            img = img.convert(\"RGBA\")\n",
    "            img_np = np.array(img, dtype=np.float32)\n",
    "\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img_np = self.transform(img_np)\n",
    "\n",
    "        return img_np, np.float32(target)\n",
    "\n",
    "\n",
    "\n",
    "# Define Transforms for making the data compatible with pytorch\n",
    "class ToTensor:\n",
    "    \"\"\"Convert a numpy array (H, W, C) to a PyTorch tensor of shape (C, H, W).\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        sample_tensor = torch.from_numpy(sample).permute(2, 0, 1)  # (C,H,W)\n",
    "        return sample_tensor\n",
    "\n",
    "class MinMaxScale:\n",
    "    \"\"\"\n",
    "    Scale each pixel channel-wise from [min_val, max_val] to [0,1].\n",
    "    Adjust min_val/max_val to match your data's range.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_val=0.0, max_val=255.0):\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def __call__(self, sample_tensor):\n",
    "        return (sample_tensor - self.min_val) / (self.max_val - self.min_val + 1e-8)\n",
    "\n",
    "\n",
    "# Compose transforms\n",
    "transform = T.Compose([\n",
    "    ToTensor(),           \n",
    "    MinMaxScale(0, 255),  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split + apply Dataloaders\n",
    "\n",
    "# TODO adapt to our data\n",
    "images_filenames = [\n",
    "    # \"path/to/image1.png\",\n",
    "    # \"path/to/image2.png\",\n",
    "    # ...\n",
    "]\n",
    "targets = [\n",
    "    # poverty_value1,\n",
    "    # poverty_value2,\n",
    "    # ...\n",
    "]\n",
    "\n",
    "# Split data\n",
    "train_files, test_files, train_targets, test_targets = train_test_split(\n",
    "    images_filenames, targets, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = PovertyDataset(train_files, train_targets, transform=transform)\n",
    "test_dataset  = PovertyDataset(test_files,  test_targets,  transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "\n",
    "# ResNet with a Small MLP Head\n",
    "# (we can also try other models like EfficientNet, but lets start with ResNet)\n",
    "class ResNetRegressor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Load a pretrained ResNet (e.g., ResNet18)\n",
    "        self.backbone = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Modify the first conv layer to accept 4 channels instead of 3\n",
    "        old_weights = self.backbone.conv1.weight.data\n",
    "        new_conv = nn.Conv2d(\n",
    "            in_channels=4, out_channels=64,\n",
    "            kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "        # Copy the original RGB weights\n",
    "        new_conv.weight.data[:, :3, :, :] = old_weights\n",
    "        # Initialize the 4th channel\n",
    "        nn.init.xavier_normal_(new_conv.weight.data[:, 3:, :, :])\n",
    "        self.backbone.conv1 = new_conv\n",
    "\n",
    "        # Remove the original FC (classifier) and replace with Identity\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        # A small MLP to map the 512-dim features to a single scalar\n",
    "        # (ResNet18 typically ends with a 512-dim feature after global pooling)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)  # single output for regression\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features via the CNN backbone\n",
    "        features = self.backbone(x)  # shape [batch_size, 512]\n",
    "        # Pass features through the MLP\n",
    "        out = self.mlp(features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Part\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetRegressor(pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device).view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # ---- EVAL ----\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device).view(-1, 1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_test_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(epoch_test_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f} | Test Loss: {epoch_test_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Plot Training vs Test Loss\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training vs. Test Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save Model Weights\n",
    "save_path = \"path/name.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model weights saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Code\n",
    "\n",
    "# Recreate the model architecture\n",
    "model_inference = ResNetRegressor(pretrained=False).to(device)\n",
    "# Load weights\n",
    "model_inference.load_state_dict(torch.load(save_path, map_location=device))\n",
    "model_inference.eval()\n",
    "\n",
    "# Helper function for single-image inference\n",
    "def predict_poverty(model, image_path, transform=None):\n",
    "    \"\"\"\n",
    "    Given a path to a 4-channel image, returns the model's predicted poverty value.\n",
    "    \"\"\"\n",
    "    with Image.open(image_path) as img:\n",
    "        # Convert to RGBA to ensure 4 channels if your file supports it\n",
    "        img = img.convert(\"RGBA\")\n",
    "        img_np = np.array(img, dtype=np.float32)\n",
    "\n",
    "    if transform is not None:\n",
    "        # Apply the same transformations used during training\n",
    "        img_tensor = transform(img_np)\n",
    "    else:\n",
    "        # Fallback: Just convert to tensor, if no transform is provided\n",
    "        img_tensor = torch.from_numpy(img_np).permute(2, 0, 1)\n",
    "\n",
    "    # Add batch dimension -> shape: (1, C, H, W)\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    # Disable gradient calculation during inference\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "    \n",
    "    # Since output is shape [1, 1], extract the scalar\n",
    "    return output.item()\n",
    "\n",
    "# EXAMPLE USAGE\n",
    "new_image_path = \"path/to/new_image.png\"\n",
    "\n",
    "# Use the same 'transform' as used in training for identical scaling\n",
    "# We can use this function in a loop to do batch inference as well\n",
    "predicted_value = predict_poverty(model_inference, new_image_path, transform=transform)\n",
    "print(f\"Predicted poverty value for '{new_image_path}': {predicted_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
