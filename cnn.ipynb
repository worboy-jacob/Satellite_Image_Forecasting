{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler  # or StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 1. Custom Dataset\n",
    "# -----------------------\n",
    "class PovertyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Example dataset structure:\n",
    "    - images_dir: Path to folder containing images. Each image has 4 channels (RGB + vegetation).\n",
    "    - targets: A list/array of the poverty values (regression targets), \n",
    "               aligned with the images in images_dir by index or filename.\n",
    "\n",
    "    For demonstration, we assume:\n",
    "      - 'images_filenames' is a list of image file paths\n",
    "      - 'targets' is a list/array of target poverty values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, images_filenames, targets, transform=None):\n",
    "        self.images_filenames = images_filenames\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image (4 channel). \n",
    "        # If your files are not standard image formats, you may need special loaders (e.g. tif)\n",
    "        img_path = self.images_filenames[idx]\n",
    "        with Image.open(img_path) as img:\n",
    "            # Ensure the image is in \"RGBA\" or 4-channel mode if needed\n",
    "            # If you have a separate vegetation channel, you might combine them manually\n",
    "            # or read a 4-band GeoTIFF.\n",
    "            # For demonstration, we assume a 4-channel image is read directly.\n",
    "            img = img.convert(\"RGBA\")  # or keep it as 4-ch if your PIL is reading a 4-band image\n",
    "\n",
    "            # Convert to numpy array (H, W, C)\n",
    "            img_np = np.array(img, dtype=np.float32)\n",
    "            \n",
    "            # If the vegetation channel is separate, you'll have to read that separately \n",
    "            # and stack them. Or if it's already included as the 4th band, skip this step.\n",
    "\n",
    "        # target\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        # Apply transform if available (including scaling, etc.)\n",
    "        if self.transform:\n",
    "            img_np = self.transform(img_np)  # e.g. transforms on numpy or tensor\n",
    "\n",
    "        return img_np, np.float32(target)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 2. Define Transforms\n",
    "# -----------------------\n",
    "class ToTensor:\n",
    "    \"\"\"Convert a numpy array (H,W,C) to a PyTorch tensor of shape (C,H,W).\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        # sample shape is (H, W, C)\n",
    "        sample_tensor = torch.from_numpy(sample).permute(2, 0, 1)  # (C, H, W)\n",
    "        return sample_tensor\n",
    "\n",
    "# Example of a scaling transform using MinMax\n",
    "class MinMaxScale:\n",
    "    \"\"\"\n",
    "    This transform scales each channel to [0,1] (or a certain range).\n",
    "    Alternatively, you could do channel-wise normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_val=None, max_val=None):\n",
    "        # If you know min/max across dataset, set them here\n",
    "        # or compute them in an offline step\n",
    "        self.min_val = min_val if min_val is not None else 0.0\n",
    "        self.max_val = max_val if max_val is not None else 255.0\n",
    "\n",
    "    def __call__(self, sample_tensor):\n",
    "        return (sample_tensor - self.min_val) / (self.max_val - self.min_val + 1e-8)\n",
    "\n",
    "# Compose the transforms\n",
    "transform = T.Compose([\n",
    "    ToTensor(),           # convert HWC -> CHW\n",
    "    MinMaxScale(0, 255),  # scale pixel values\n",
    "])\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 3. Prepare Data & Splits\n",
    "# -----------------------\n",
    "\n",
    "# Suppose you have these lists from your data pipeline\n",
    "# images_filenames = [\"path/to/image1.png\", \"path/to/image2.png\", ...]\n",
    "# targets = [poverty_value1, poverty_value2, ...]\n",
    "\n",
    "images_filenames = [...]  # fill in\n",
    "targets = [...]           # fill in\n",
    "\n",
    "# Using sklearn to split into train/test\n",
    "train_files, test_files, train_targets, test_targets = train_test_split(\n",
    "    images_filenames, targets, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = PovertyDataset(train_files, train_targets, transform=transform)\n",
    "test_dataset  = PovertyDataset(test_files,  test_targets,  transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 4. Define/Modify Pretrained Model\n",
    "# -----------------------\n",
    "# Weâ€™ll use ResNet18 here, but you can pick other architectures.\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# 4.1 Modify the first conv layer to accept 4 channels\n",
    "# The original resnet18 first layer is:\n",
    "#   model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# We change it to have in_channels=4\n",
    "old_weights = model.conv1.weight.data  # shape: (64, 3, 7, 7)\n",
    "\n",
    "# Create a new conv layer\n",
    "new_conv = nn.Conv2d(\n",
    "    in_channels=4,        # 4 channels now\n",
    "    out_channels=64,\n",
    "    kernel_size=7,\n",
    "    stride=2,\n",
    "    padding=3,\n",
    "    bias=False\n",
    ")\n",
    "\n",
    "# Copy the original RGB weights\n",
    "# old_weights[:, :3, :, :] is shape: (64, 3, 7, 7)\n",
    "new_conv.weight.data[:, :3, :, :] = old_weights\n",
    "# Initialize the weights of the 4th channel to something small (e.g. zeros or random)\n",
    "nn.init.xavier_normal_(new_conv.weight.data[:, 3:, :, :])\n",
    "\n",
    "model.conv1 = new_conv\n",
    "\n",
    "# 4.2 Replace the final classification layer (fc) with a regression head\n",
    "# Original fully connected layer: model.fc = nn.Linear(512, 1000)\n",
    "# For regression, let's output a single value\n",
    "num_feats = model.fc.in_features\n",
    "model.fc = nn.Linear(num_feats, 1)  # single regression output\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 5. Set up Training\n",
    "# -----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 5  # for example\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# -----------------------\n",
    "# 6. Training Loop\n",
    "# -----------------------\n",
    "for epoch in range(num_epochs):\n",
    "    # --- TRAIN ---\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device).view(-1, 1)  # shape (batch_size, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # --- EVAL ---\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device).view(-1, 1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_test_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(epoch_test_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f} | Test Loss: {epoch_test_loss:.4f}\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 7. Plotting\n",
    "# -----------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs+1), test_losses, label='Test Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train vs Test Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
